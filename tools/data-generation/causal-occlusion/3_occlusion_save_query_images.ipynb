{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start 3_occlusion_save_query_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save extremal query images of occlusion stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import cv2\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import occlusion_utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-s\", \"--stimuli-dir\", required=True, help=\"Path to save stimuli to.\")\n",
    "parser.add_argument(\"-t\", \"--trial-type\", required=True, help=\"instruction_practice_catch or sampled_trials.\")\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_dir = args.stimuli_dir\n",
    "trial_type = args.trial_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load experiment specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in unit specifications from csv into pandas dataframe\n",
    "path_to_csv_file = os.path.join(stimuli_dir, f\"layer_folder_mapping_{trial_type}.csv\")\n",
    "unit_specs_df = pd.read_csv(path_to_csv_file, header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_specs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_data(stimulus_type, percentage_side_length_i):\n",
    "    npy_file_name = f\"activations_for_occlusions_of_{percentage_side_length_i}_percent.npy\"\n",
    "    path_to_npy = os.path.join(\n",
    "        data_dir,\n",
    "        f\"{percentage_side_length_i}_percent_side_length\",\n",
    "        npy_file_name\n",
    "    )\n",
    "    activations_data = np.load(path_to_npy)\n",
    "    return activations_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in unit_specs_df.iterrows():\n",
    "    \n",
    "    # load unit specification\n",
    "    layer_number = row[\"layer_number\"]\n",
    "    kernel_size_number = row[\"kernel_size_number\"]\n",
    "    channel_number = row[\"channel_number\"]\n",
    "    feature_map_number = row[\"feature_map_number\"]\n",
    "    layer_name = row[\"layer_name\"]\n",
    "    pre_post_relu = row[\"pre_post_relu\"]\n",
    "    \n",
    "    print(row)\n",
    "    \n",
    "    for batch in range(ut.n_batches):\n",
    "        start = time.time()\n",
    "    \n",
    "        # load images\n",
    "        data_dir = os.path.join(\n",
    "            stimuli_dir,\n",
    "            ut.objective,\n",
    "            trial_type,\n",
    "            f\"layer_{layer_number}\",\n",
    "            f\"kernel_size_{kernel_size_number}\",\n",
    "            f\"channel_{channel_number}\",\n",
    "            \"natural_images\",\n",
    "            f\"batch_{batch}\"\n",
    "        )\n",
    "        data_loader = ut.get_data_loader(os.path.join(data_dir, \"val\"))\n",
    "        image, _, _ = next(iter(data_loader))\n",
    "        image_np_transformed = image.numpy().transpose(0,2,3,1) # (1, 224, 224, 3)\n",
    "        \n",
    "        # loop through occlusion sizes\n",
    "        for percentage_side_length_i, occlusion_size_i, heatmap_size_i in zip(\n",
    "            ut.percentage_side_length_list, \n",
    "            ut.occlusion_sizes_list, \n",
    "            ut.heatmap_sizes_list):\n",
    "            print(f\"percentage_side_length_i {percentage_side_length_i}, occlusion_size_i {occlusion_size_i}, heatmap_size_i {heatmap_size_i}\")\n",
    "\n",
    "            activations_data_one_occlusion_size = get_activations_data(\"occlusions\", percentage_side_length_i)\n",
    "\n",
    "            query_dir = os.path.join(\n",
    "                data_dir,\n",
    "                f\"{percentage_side_length_i}_percent_side_length\"\n",
    "            )\n",
    "            os.makedirs(query_dir, exist_ok=True)\n",
    "\n",
    "            list_of_positions = ut.get_list_of_occlusion_positions(heatmap_size_i, occlusion_size_i)\n",
    "\n",
    "            # loop through query images\n",
    "            for query_type_i in [\"default\", \"max_activation\", \"min_activation\"]:\n",
    "\n",
    "                # get images\n",
    "                image_to_be_saved = copy.deepcopy(image_np_transformed.squeeze())\n",
    "\n",
    "                # add patch to occlusion images\n",
    "                if \"activation\" in query_type_i:\n",
    "                    if \"min\" in query_type_i:\n",
    "                        extreme_idx = np.argmin(activations_data_one_occlusion_size[:-1])\n",
    "                    elif \"max\" in query_type_i:\n",
    "                        extreme_idx = np.argmax(activations_data_one_occlusion_size[:-1])\n",
    "                    x_start, x_end, y_start, y_end = list_of_positions[extreme_idx]\n",
    "\n",
    "                    # add occlusion\n",
    "                    image_to_be_saved[x_start:x_end, y_start:y_end, :] = np.mean(np.mean(image_to_be_saved[x_start:x_end, y_start:y_end, :], axis=0), axis=0)\n",
    "                                    \n",
    "                image_path = os.path.join(query_dir, f\"query_{query_type_i}.png\")\n",
    "                cv2.imwrite(image_path, cv2.cvtColor(image_to_be_saved*255, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "                plt.imshow(image_to_be_saved)\n",
    "                plt.show()\n",
    "        end = time.time()\n",
    "        print(f\"       time for one batch: {end-start}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done with 3_occlusion_save_extremal_query_images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}