{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"starting 2_occlusion_activations_in_Inception_V1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Occlusion Stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lucid imports\n",
    "import lucid.modelzoo.vision_models as models\n",
    "from render import import_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom imports\n",
    "import occlusion_utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import InceptionV1 from the Lucid modelzoo\n",
    "model = models.InceptionV1()\n",
    "model.load_graphdef()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_forward_pass = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seeds\n",
    "tf.set_random_seed(1234)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-s\", \"--stimuli-dir\", required=True, help=\"Path to save stimuli to.\")\n",
    "parser.add_argument(\"-t\", \"--trial-type\", required=True, help=\"instruction_practice_catch or sampled_trials.\")\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_dir = args.stimuli_dir\n",
    "trial_type = args.trial_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load experiment specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in unit specifications from csv into pandas dataframe\n",
    "path_to_csv_file = os.path.join(stimuli_dir, f\"layer_folder_mapping_{trial_type}.csv\")\n",
    "unit_specs_df = pd.read_csv(path_to_csv_file, header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unit_specs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, row in unit_specs_df.iterrows():\n",
    "    \n",
    "    # load unit specification\n",
    "    layer_number = row[\"layer_number\"]\n",
    "    kernel_size_number = row[\"kernel_size_number\"]\n",
    "    channel_number = row[\"channel_number\"]\n",
    "    feature_map_number = row[\"feature_map_number\"]\n",
    "    layer_name = row[\"layer_name\"]\n",
    "    pre_post_relu = row[\"pre_post_relu\"]\n",
    "    \n",
    "    print(row)\n",
    "    \n",
    "    for batch in range(ut.n_batches):\n",
    "        print(f\"batch {batch}\")\n",
    "        \n",
    "        start = time.time()\n",
    "    \n",
    "        # dataloader\n",
    "        data_dir = os.path.join(\n",
    "            stimuli_dir,\n",
    "            ut.objective,\n",
    "            trial_type,\n",
    "            f\"layer_{layer_number}\",\n",
    "            f\"kernel_size_{kernel_size_number}\",\n",
    "            f\"channel_{channel_number}\",\n",
    "            \"natural_images\",\n",
    "            f\"batch_{batch}\",\n",
    "        )\n",
    "        data_loader = ut.get_data_loader(os.path.join(data_dir, \"val\"))\n",
    "\n",
    "        for heatmap_size_i, occlusion_size_i, percentage_side_length_i in zip(ut.heatmap_sizes_list, ut.occlusion_sizes_list, ut.percentage_side_length_list):\n",
    "            print(heatmap_size_i, occlusion_size_i, percentage_side_length_i)\n",
    "\n",
    "            list_of_positions = ut.get_list_of_occlusion_positions(heatmap_size_i, occlusion_size_i)\n",
    "\n",
    "            if 'session' in locals() and session is not None:\n",
    "                print('Close interactive session')\n",
    "                session.close()\n",
    "\n",
    "            with tf.Graph().as_default() as graph, tf.Session() as sess:\n",
    "\n",
    "                image = tf.placeholder(tf.float32, shape=(batch_size_forward_pass, 224, 224, 3))\n",
    "                print(\"image.shape\", image.shape)\n",
    "                model_instance = import_model(model, image)\n",
    "                tf_activations_list, unique_layer_str_list = ut.get_tf_activations_list(model_instance, layer_name, pre_post_relu)\n",
    "\n",
    "                # iterate over all occlusion positions in batches\n",
    "                list_of_activations_for_occlusions = []\n",
    "                n_iterations = math.ceil(len(list_of_positions) / batch_size_forward_pass)\n",
    "                for iteration_i in range(n_iterations):\n",
    "                    cur_list_of_positions = list_of_positions[iteration_i*batch_size_forward_pass:iteration_i*batch_size_forward_pass+batch_size_forward_pass]\n",
    "\n",
    "                    images, targets, paths = next(iter(data_loader))\n",
    "\n",
    "                    # forward pass\n",
    "                    images_np_transformed = images.numpy().transpose(0,2,3,1) # (1, 224, 224, 3)\n",
    "                    images_np_transformed_copied = copy.deepcopy(images_np_transformed)\n",
    "                    images_np_transformed_ready_for_occlusion = np.tile(images_np_transformed_copied, (batch_size_forward_pass, 1, 1, 1)) # (batch_size_forward_pass, 224, 224, 3)\n",
    "\n",
    "                    # loop through occlusion positions\n",
    "                    for idx, (x_start, x_end, y_start, y_end) in enumerate(cur_list_of_positions):\n",
    "                        images_np_transformed_ready_for_occlusion[idx, x_start:x_end, y_start:y_end, :] = np.mean(np.mean(images_np_transformed_ready_for_occlusion[idx, x_start:x_end, y_start:y_end, :], axis=0), axis=0)\n",
    "\n",
    "#                     # plot patches to check if occlusion worked fine\n",
    "#                     plt.imshow(images_np_transformed_ready_for_occlusion[idx, :, :, :])\n",
    "#                     plt.show()\n",
    "\n",
    "                    activations_list = sess.run(tf_activations_list, {image: images_np_transformed_ready_for_occlusion})\n",
    "    \n",
    "                    # unpack single list item\n",
    "                    activations_np = activations_list[0]\n",
    "                    unit_activations = ut.get_activation_according_to_objective(ut.objective, activations_np, feature_map_number)\n",
    "                    list_of_activations_for_occlusions.append(list(unit_activations))\n",
    "\n",
    "                # after having calculated all occlusions: save activations\n",
    "                final_list_of_activations_for_occlusions = [item for sublist in list_of_activations_for_occlusions for item in sublist]\n",
    "                destination_dir = os.path.join(data_dir, f\"{percentage_side_length_i}_percent_side_length\")\n",
    "                os.makedirs(destination_dir, exist_ok=True)\n",
    "                filename = os.path.join(destination_dir, f\"activations_for_occlusions_of_{percentage_side_length_i}_percent.npy\")\n",
    "                np.save(filename, final_list_of_activations_for_occlusions[:len(list_of_positions)+1])\n",
    "                print(f\"activations saved under{filename}\")\n",
    "\n",
    "                print('Done!!!')\n",
    "        end = time.time()\n",
    "        print(f\"       time for one batch: {end-start}\")\n",
    "print(\"Completely done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done with 2_occlusion_activations_in_Inception_V1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
