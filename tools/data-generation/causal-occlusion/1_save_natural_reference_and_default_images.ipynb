{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"starting 1_save_natural_reference_images_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook saves ut.n_batches of ut.n_bins (== batch-size) most strongly activating images as well as the default ImageNet images for the occlusion stimuli on share.\n",
    "\n",
    "\n",
    "Large parts of this notebook are taken from https://github.com/bethgelab/testing_visualizations/blob/master/generate_stimuli/save_natural_stimuli.py on 14.04.2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import time\n",
    "from torch.utils import data\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision.datasets.folder import default_loader as default_image_loader\n",
    "from torchvision import transforms\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import occlusion_utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_max_filename = \"activation_max.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-s\", \"--stimuli-dir\", required=True, help=\"Path to save stimuli to.\")\n",
    "parser.add_argument(\"-t\", \"--trial-type\", required=True, help=\"instruction_practice_catch or sampled_trials.\")\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_dir = args.stimuli_dir\n",
    "trial_type = args.trial_type\n",
    "n_reference_images, n_bins, _ = ut.get_number_of_stimuli(stimuli_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in units (layer, kernel_size, feature map) of interest\n",
    "path_to_csv_file = os.path.join(stimuli_dir, f\"layer_folder_mapping_{trial_type}.csv\")\n",
    "print(f\"read in files from {path_to_csv_file}\")\n",
    "unit_specs_df = pd.read_csv(path_to_csv_file, header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_specs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_crop_transform = transforms.Compose(\n",
    "        [transforms.Resize(256), transforms.CenterCrop(224)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFileListDataSet(data.Dataset):\n",
    "    def __init__(self, file_list, transform=None, target_transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.loader = default_image_loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        impath = self.file_list[index]\n",
    "        img = self.loader(impath)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_randomized_indices_except_for_last_bin(n_bins, n_batches, seed):\n",
    "    \"\"\"generate randomized order of indices except for the last bin.\n",
    "    This means that for all but the last bin the 20 images that belong to one \n",
    "    bin (e.g. min_0.png) is different:\n",
    "    randomize(0...19), then randomize(20...39) ... randomize(160...179).\n",
    "    The last bin stays in order: (180 181 182 ... 199).\n",
    "    Use this function for between-subject designs!!!\n",
    "    \"\"\"\n",
    "    randomized_indices = np.empty([n_bins * n_batches])\n",
    "    array_of_n_batches = np.arange(n_batches)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    for bin_i in range(n_bins):\n",
    "        # keep the order of the largest indices in order\n",
    "        if bin_i == n_bins - 1:\n",
    "            randomized_indices[\n",
    "                (n_batches * bin_i) : (n_batches + bin_i * n_batches)\n",
    "            ] = array_of_n_batches + bin_i * n_batches\n",
    "        # permute the order of all but the largest indices\n",
    "        else:\n",
    "            randomized_indices[\n",
    "                (n_batches * bin_i) : (n_batches + bin_i * n_batches)\n",
    "            ] = np.random.permutation(array_of_n_batches + bin_i * n_batches)\n",
    "\n",
    "    return randomized_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy images over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names_number_dict = {\n",
    "    'mixed3a': 0,\n",
    "    'mixed3b': 1,\n",
    "    'mixed4a': 2,\n",
    "    'mixed4b': 3,\n",
    "    'mixed4c': 4,\n",
    "    'mixed4d': 5,\n",
    "    'mixed4e': 6,\n",
    "    'mixed5a': 7,\n",
    "    'mixed5b': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over dataframe\n",
    "for _, row in tqdm(unit_specs_df.iterrows()):\n",
    "    start = time.time()\n",
    "    kernel_size = row[\"kernel_size_number\"]\n",
    "    channel_number = row[\"channel_number\"]\n",
    "    feature_map = row[\"feature_map_number\"]\n",
    "    layer_number = row[\"layer_number\"]\n",
    "    layer_name = row[\"layer_name\"]\n",
    "    print(f\"layer_name {layer_name}, feature_map {feature_map}, channel {channel_number}, kernel_size {kernel_size}\")\n",
    "    \n",
    "    # folder where csv with activations for all 50,000 images is stored\n",
    "    csv_filename = f\"$DATAPATH/all_activations_imagenet_train/channel/sampled_trials/layer_{layer_names_number_dict[layer_name]}/activations_whole_dataset.pkl\"\n",
    "    \n",
    "    # load this csv into df (takes long!)\n",
    "    print(\"Loading df\")\n",
    "    # df = pd.read_csv(csv_filename, header=1, converters={'activation': ast.literal_eval})\n",
    "    df = pd.read_pickle(csv_filename)\n",
    "    \n",
    "    print(\"df loaded\")\n",
    "    \n",
    "    # select relevant feature map and sort in descending order\n",
    "    df_expanded = df.copy()\n",
    "    df_expanded[\"selected_activation\"] = df[\"activation\"].apply(lambda x: x[feature_map])\n",
    "    df_expanded_sorted = df_expanded.sort_values(\"selected_activation\", ascending=True)\n",
    "    \n",
    "    # create dataframes with relevant columns and rows only. Also, randomize the order in one image bin\n",
    "    assert n_bins <= 10, \"n_bins is upper bounded by 10\"\n",
    "\n",
    "    # generate indices always under the assumption that we need 10 bins\n",
    "    max_indices = get_randomized_indices_except_for_last_bin(\n",
    "        10, ut.n_batches, seed=feature_map + 1\n",
    "    )\n",
    "    # then only use last n_bins bins and shift their index values\n",
    "    max_indices = max_indices[-n_bins*ut.n_batches:] - ((10 - n_bins) * ut.n_batches)\n",
    "    max_images_activations_df = (\n",
    "        df_expanded_sorted[-n_bins * ut.n_batches :]\n",
    "        .drop([\"activation\", \"target class\"], axis=1)\n",
    "        .iloc[max_indices]\n",
    "    )\n",
    "    \n",
    "    # load dataset\n",
    "    max_file_names = max_images_activations_df[\"path to image\"].tolist()\n",
    "    max_dataset = ImageFileListDataSet(\n",
    "        max_file_names, transform=center_crop_transform\n",
    "    )\n",
    "    \n",
    "    for image_idx_in_batch in tqdm(range(n_bins), position=1, leave=False):\n",
    "        for batch in tqdm(range(ut.n_batches), position=2, leave=False):\n",
    "            image_idx = batch + ut.n_batches * image_idx_in_batch\n",
    "\n",
    "            this_activation = max_images_activations_df[\"selected_activation\"].iloc[image_idx]\n",
    "\n",
    "            # save image\n",
    "            max_image = max_dataset[image_idx]\n",
    "            output_dir = os.path.join(\n",
    "                stimuli_dir,\n",
    "                \"channel\",\n",
    "                trial_type,\n",
    "                f\"layer_{layer_number}\",\n",
    "                f\"kernel_size_{kernel_size}\",\n",
    "                f\"channel_{channel_number}\",\n",
    "                \"natural_images\",\n",
    "                f\"batch_{batch}\",\n",
    "            )\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            max_filename = os.path.join(output_dir, f\"reference_max_{image_idx_in_batch}.png\")\n",
    "            max_image.save(max_filename)\n",
    "\n",
    "\n",
    "            # save activation to csv\n",
    "            # initialize csv file if it does not exist yet\n",
    "            if image_idx_in_batch == 0:\n",
    "                with open(\n",
    "                    os.path.join(output_dir, csv_max_filename), \"w\"\n",
    "                ) as csvFile:\n",
    "                    csv_writer = csv.writer(\n",
    "                        csvFile, delimiter=\",\", lineterminator=\"\\n\"\n",
    "                    )\n",
    "                    csv_writer.writerow([\"image_path\", \"idx\", \"activation\"])\n",
    "                csvFile.close()\n",
    "            # fill csv file\n",
    "            with open(os.path.join(output_dir, csv_max_filename), \"a\") as csvFile:\n",
    "                csv_writer = csv.writer(csvFile, delimiter=\",\", lineterminator=\"\\n\")\n",
    "                csv_writer.writerow(\n",
    "                    [\n",
    "                        max_images_activations_df[\"path to image\"].iloc[image_idx],\n",
    "                        image_idx_in_batch,\n",
    "                        max_images_activations_df[\"selected_activation\"].iloc[image_idx],\n",
    "                    ]\n",
    "                )\n",
    "            csvFile.close()\n",
    "            \n",
    "            # save unprocessed image for occlusion calculations\n",
    "            if image_idx_in_batch == n_reference_images:\n",
    "                img_file = max_images_activations_df[\"path to image\"].iloc[image_idx]\n",
    "\n",
    "                destination_dir = os.path.join(\n",
    "                    stimuli_dir,\n",
    "                    \"channel\", \n",
    "                    trial_type,\n",
    "                    f\"layer_{layer_number}\", \n",
    "                    f\"kernel_size_{kernel_size}\", \n",
    "                    f\"channel_{channel_number}\", \n",
    "                    \"natural_images\",\n",
    "                    f\"batch_{batch}\",\n",
    "                    \"val\",\n",
    "                    img_file.split(os.path.sep)[-2]) #n...\n",
    "                os.makedirs(destination_dir, exist_ok=True)\n",
    "                print(f\"destination_dir {destination_dir}\")\n",
    "                shutil.copy(img_file, destination_dir)\n",
    "                \n",
    "    end = time.time()\n",
    "    print(f\"       time for one layer: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished 1_save_natural_reference_images_new\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
