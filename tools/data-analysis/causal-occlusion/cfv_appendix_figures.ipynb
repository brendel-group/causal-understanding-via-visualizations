{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "threatened-guard",
   "metadata": {},
   "source": [
    "This notebook produces figures related to the counterfactual-inspired experiment for the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-stomach",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import from mturk folder\n",
    "import os, sys, inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "mturkdir = os.path.join(os.path.dirname(os.path.dirname(currentdir)), \"mturk\")\n",
    "sys.path.insert(0, mturkdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-frame",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mturk import RepeatedTaskResult\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_figures as utf\n",
    "import utils_figures_helper as utf_helper\n",
    "import utils_MTurk_figures as utf_mturk\n",
    "import utils_data as utd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-prison",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder containing the pkl files generated by the experiment's code\n",
    "raw_results_folder = \"data/counterfactual_experiment\"\n",
    "\n",
    "save_csv = False\n",
    "include_baselines = True\n",
    "\n",
    "# START for figures\n",
    "save_fig = False\n",
    "# name of the folder in ./figures/ where all resulting figures will be saved\n",
    "exp_str = \"counterfactual_experiment\"\n",
    "instr_type_list = [\"optimized\", \"natural\", \"mixed\", \"blur\", \"none\"]\n",
    "branches_labels_list = [\"3x3\", \"pool\"]\n",
    "kernel_size_list = [\"1\", \"3\"]\n",
    "# END for figures\n",
    "\n",
    "# START for payment\n",
    "mturk_payment_one_HIT = 2.34\n",
    "mturk_payment_one_HIT_none = 0.84\n",
    "repetition_factor_due_to_exclusion = 1.35\n",
    "expected_distinct_workers = 50\n",
    "# END for payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_labels = {\n",
    "    \"optimized\": \"Synthetic\",\n",
    "    \"natural\": \"Natural\",\n",
    "    \"mixed\": \"Mixed\",\n",
    "    \"none\": \"None\",\n",
    "    \"blur\": \"Blur\",\n",
    "}\n",
    "labels = [instruction_labels[it] for it in instr_type_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-indicator",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-directory",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if the `calculate_relative_activation_difference.py` script was already run for all json configurations\n",
    "If not, do so.\n",
    "to add the query activation information to the structure\n",
    "save the resulting files with the filenames shown below\n",
    "\"\"\"\n",
    "\n",
    "if include_baselines:\n",
    "    structure_json_map = {\n",
    "        \"natural\": \"natural_with_baselines.json\",\n",
    "        \"optimized\": \"optimized_with_baselines.json\",\n",
    "        \"mixed\": \"mixed_with_baselines.json\",\n",
    "        \"blur\": \"natural_blur_with_baselines.json\",\n",
    "        \"none\": \"natural_with_baselines.json\",\n",
    "    }\n",
    "else:\n",
    "    structure_json_map = {\n",
    "        \"natural\": \"natural.json\",\n",
    "        \"optimized\": \"optimized.json\",\n",
    "        \"mixed\": \"mixed.json\",\n",
    "        \"blur\": \"natural_blur.json\",\n",
    "        \"none\": \"natural.json\",\n",
    "    }\n",
    "\n",
    "trial_structures = utd.load_and_parse_trial_structure(\n",
    "    raw_results_folder, [structure_json_map[it] for it in instr_type_list]\n",
    ")\n",
    "trial_structures = {k: v for k, v in zip(instr_type_list, trial_structures)}\n",
    "df, df_checks, df_feedback = utd.load_and_parse_all_results(\n",
    "    raw_results_folder, instr_type_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-livestock",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add a column to the result df indicating whether the row belongs to an excluded or included response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-turner",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_map_excluded_responses(column_name=\"passed_checks\"):\n",
    "    def map_excluded_responses(row):\n",
    "        rows = df_checks[\n",
    "            (df_checks[\"task_id\"] == row[\"task_id\"])\n",
    "            & (df_checks[\"response_index\"] == row[\"response_index\"])\n",
    "        ]\n",
    "        result = not rows[column_name].item()\n",
    "        return result\n",
    "\n",
    "    return map_excluded_responses\n",
    "\n",
    "\n",
    "df[\"excluded_response\"] = df.apply(get_map_excluded_responses(\"passed_checks\"), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-pathology",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a unique column based on task id and response id (unique within each task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-marathon",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df, df_checks = utd.add_task_response_id(df, df_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-renaissance",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_main = (\n",
    "    df[(df[\"catch_trial\"] == False) & (df[\"is_demo\"] == False)]\n",
    "    .reset_index()\n",
    "    .drop(\"index\", axis=1)\n",
    ")\n",
    "df_catch_trials = (\n",
    "    df[(df[\"catch_trial\"] == True) & (df[\"is_demo\"] == False)]\n",
    "    .reset_index()\n",
    "    .drop(\"index\", axis=1)\n",
    ")\n",
    "df_demo_trials = df[df[\"is_demo\"] == True].reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-arthur",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Append structure information such as layer, kernel size, etc. to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-malaysia",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_main = utd.append_trial_structure_to_results(df_main, trial_structures)\n",
    "df_catch_trials = utd.append_trial_structure_to_results(\n",
    "    df_catch_trials, trial_structures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-entrance",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split data up in trials belonging to excluded responses, and those that passed the exclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_excluded = df_main[df_main[\"excluded_response\"]]\n",
    "df_main_not_excluded = df_main[~df_main[\"excluded_response\"]]\n",
    "\n",
    "df_catch_trials_excluded = df_catch_trials[df_catch_trials[\"excluded_response\"]]\n",
    "df_catch_trials_not_excluded = df_catch_trials[~df_catch_trials[\"excluded_response\"]]\n",
    "\n",
    "df_demo_trials_excluded = df_demo_trials[df_demo_trials[\"excluded_response\"]]\n",
    "df_demo_trials_not_excluded = df_demo_trials[~df_demo_trials[\"excluded_response\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-heath",
   "metadata": {},
   "source": [
    "Calculate how often the demo trials had to be repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checks = utd.checks_add_demo_trial_repetitions(df_demo_trials, df_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-envelope",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df, df_checks = utd.process_checks(df, df_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catch_trials_not_excluded_ignoring_catch_trials = utd.get_catch_trials_as_main_data(\n",
    "    df_catch_trials, df_checks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-parade",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_checks_not_excluded = df_checks[df_checks[\"passed_checks\"]]\n",
    "df_checks_excluded = df_checks[~df_checks[\"passed_checks\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-minutes",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if save_csv:\n",
    "    # save dataframes to csv\n",
    "    df_checks.to_csv(os.path.join(raw_results_folder, \"df_exclusion_criteria.csv\"))\n",
    "    df.to_csv(os.path.join(raw_results_folder, \"df_trials.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-stress",
   "metadata": {},
   "source": [
    "# Plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-lawsuit",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figures_folder = os.path.join(\n",
    "    \"figures\", exp_str\n",
    ")\n",
    "if save_fig:\n",
    "    os.makedirs(figures_folder, exist_ok=True)\n",
    "    print(\"Saving results to\", figures_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-dancing",
   "metadata": {},
   "source": [
    "# Figure 8: Overlap between labes of query and reference images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-realtor",
   "metadata": {},
   "source": [
    "Please see the following file to produce is figure:`tools/data-generation/causal-occlusion/C_get_labels_of_natural_reference_and_default_images.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-clear",
   "metadata": {},
   "source": [
    "# Figure 12: Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf.make_plot_natural_are_better_wrt_confidence(\n",
    "    df_main_not_excluded,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=save_fig,\n",
    "    instr_type_list=instr_type_list,\n",
    "    conditioned_on=None,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "utf.make_plot_natural_are_better_wrt_confidence(\n",
    "    df_main_not_excluded,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=save_fig,\n",
    "    instr_type_list=instr_type_list,\n",
    "    conditioned_on=\"correctness\",\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "utf.make_plot_natural_are_better_wrt_confidence(\n",
    "    df_main_not_excluded,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=save_fig,\n",
    "    instr_type_list=instr_type_list,\n",
    "    conditioned_on=\"falseness\",\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-contrast",
   "metadata": {},
   "source": [
    "# Figure 13: Reaction Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf.make_plot_natural_are_better_wrt_reaction_time(\n",
    "    df_main_not_excluded,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=True,\n",
    "    instr_type_list=instr_type_list,\n",
    "    conditioned_on=None,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "utf.make_plot_natural_are_better_wrt_reaction_time(\n",
    "    df_main_not_excluded,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=True,\n",
    "    instr_type_list=instr_type_list,\n",
    "    conditioned_on=\"correctness\",\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "utf.make_plot_natural_are_better_wrt_reaction_time(\n",
    "    df_main_not_excluded,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=True,\n",
    "    instr_type_list=instr_type_list,\n",
    "    conditioned_on=\"falseness\",\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-knitting",
   "metadata": {},
   "source": [
    "# Figure 14: Analyze performance for each unique batch of data of selected units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_dict = {}\n",
    "unit_dict[\"easy\"] = {\"layer\": \"7\", \"kernel_size\": \"3\", \"marker\": \"x\"}\n",
    "unit_dict[\"intermediate\"] = {\"layer\": \"6\", \"kernel_size\": \"3\", \"marker\": \"+\"}\n",
    "unit_dict[\"difficult\"] = {\"layer\": \"1\", \"kernel_size\": \"3\", \"marker\": \"3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dfs_grouped_by_batch(df, batch_ids):\n",
    "    df = df.copy()\n",
    "\n",
    "    for bid in batch_ids:\n",
    "        selected_df = df[df[\"batch\"] == bid]\n",
    "        yield selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for difficulty_i, unit_spec_i in unit_dict.items():\n",
    "    print(difficulty_i)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5)\n",
    "    fig.set_size_inches((5.4992 * 2, 5))\n",
    "\n",
    "    df = df_main_not_excluded[\n",
    "        (df_main_not_excluded[\"layer\"] == unit_spec_i[\"layer\"])\n",
    "        & (df_main_not_excluded[\"kernel_size\"] == unit_spec_i[\"kernel_size\"])\n",
    "    ].copy()\n",
    "\n",
    "    batch_ids = sorted(df[\"batch\"].unique().tolist())\n",
    "    row_i = 0\n",
    "    col_i = 0\n",
    "    for batch_idx, (batch_df) in enumerate(\n",
    "        zip(generate_dfs_grouped_by_batch(df, batch_ids))\n",
    "    ):\n",
    "\n",
    "        dict_acc = {}\n",
    "        for reference_type_i in instr_type_list:\n",
    "            df_factor_i = batch_df[0][batch_df[0][\"mode\"] == reference_type_i].copy()\n",
    "\n",
    "            if len(df_factor_i) == 0:\n",
    "                dict_acc[reference_type_i] = np.nan\n",
    "            else:\n",
    "                accuracy = (df_factor_i[\"correct\"] == True).sum() / df_factor_i.shape[0]\n",
    "                dict_acc[reference_type_i] = accuracy\n",
    "\n",
    "        # loop through conditions\n",
    "        axes[row_i, col_i].bar(\n",
    "            range(len(dict_acc)),\n",
    "            list(dict_acc.values()),\n",
    "            color=[utf.colors[it] for it in instr_type_list],\n",
    "        )\n",
    "\n",
    "        axes[row_i, col_i].axhline(\n",
    "            0.5, color=\"k\", linestyle=\"--\", linewidth=1, label=\"Chance\"\n",
    "        )\n",
    "\n",
    "        axes[row_i, col_i].set_ylim(0, 1)\n",
    "        axes[row_i, col_i].set_title(f\"Image Set {batch_idx}\")\n",
    "        if col_i == 0:\n",
    "            axes[row_i, col_i].set_ylabel(\"Proportion Correct\")\n",
    "        else:\n",
    "            axes[row_i, col_i].spines[\"left\"].set_visible(False)\n",
    "            axes[row_i, col_i].set_yticks([])\n",
    "            axes[row_i, col_i].set_yticklabels([])\n",
    "        axes[row_i, col_i].set_xticks([])\n",
    "        axes[row_i, col_i].set_xticklabels([])\n",
    "\n",
    "        # no axis on top and right\n",
    "        axes[row_i, col_i].spines[\"top\"].set_visible(False)\n",
    "        axes[row_i, col_i].spines[\"right\"].set_visible(False)\n",
    "\n",
    "        col_i += 1\n",
    "        if col_i == 5:\n",
    "            row_i = 1\n",
    "            col_i = 0\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        plot_name = f\"accuracy_per_batch\"\n",
    "        for version in range(100):\n",
    "            file_name = os.path.join(\n",
    "                figures_folder, f\"{plot_name}_{difficulty_i}_{version}.pdf\"\n",
    "            )\n",
    "            # if file_name does not yet exist, use it\n",
    "            if not os.path.exists(file_name):\n",
    "                break\n",
    "        print(f\"figure saved under {file_name}\")\n",
    "        plt.savefig(file_name, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-regulation",
   "metadata": {},
   "source": [
    "# Figure 15: Cohen's kappa per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expert data\n",
    "df_expert_baseline = pd.read_csv(\"data/baselines2/df_main_trials.csv\")\n",
    "df_expert_baseline[\"expert_baseline\"] = True\n",
    "df_expert_baseline[\"mode_extended\"] = df_expert_baseline.apply(\n",
    "    lambda row: \"e_\" + row[\"mode\"], axis=1  # e for expert\n",
    ")\n",
    "df_expert_baseline[\"kernel_size\"] = df_expert_baseline.apply(\n",
    "    lambda row: str(row[\"kernel_size\"]), axis=1\n",
    ")\n",
    "df_expert_baseline[\"layer\"] = df_expert_baseline.apply(\n",
    "    lambda row: str(row[\"layer\"]), axis=1\n",
    ")\n",
    "\n",
    "# extend worker df with new columns\n",
    "df_main_not_excluded_copy = df_main_not_excluded.copy()\n",
    "df_main_not_excluded_copy[\"expert_baseline\"] = False\n",
    "df_main_not_excluded_copy[\"mode_extended\"] = df_main_not_excluded_copy.apply(\n",
    "    lambda row: \"w_\" + row[\"mode\"], axis=1  # w for worker\n",
    ")\n",
    "df_main_not_excluded_with_expert_baseline = pd.concat(\n",
    "    (df_expert_baseline, df_main_not_excluded_copy)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# load primary object baseline\n",
    "if os.path.exists(\"data/baselines2/df_primary_object_baseline.csv\"):\n",
    "    df_primary_object_baseline = pd.read_csv(\n",
    "        \"data/baselines2/df_primary_object_baseline.csv\"\n",
    "    )\n",
    "\n",
    "    def parse_primary_object_baseline(row):\n",
    "        mask = (\n",
    "            (df_primary_object_baseline[\"batch\"] == row[\"batch\"])\n",
    "            & (df_primary_object_baseline[\"layer\"] == row[\"layer\"])\n",
    "            & (df_primary_object_baseline[\"kernel_size\"] == row[\"kernel_size\"])\n",
    "        )\n",
    "        selected_rows = df_primary_object_baseline[mask]\n",
    "        if not len(selected_rows) == 1:\n",
    "            print(\n",
    "                \"missing information for row:\",\n",
    "                row[[\"batch\", \"trial_index\", \"mode\", \"task_number\"]],\n",
    "            )\n",
    "            print()\n",
    "        return selected_rows.iloc[0][\"primary_object_choice\"]\n",
    "\n",
    "    df_main_not_excluded_with_expert_baseline[\n",
    "        \"primary_object_baseline_choice\"\n",
    "    ] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "        axis=1, func=parse_primary_object_baseline\n",
    "    )\n",
    "\n",
    "    # clean up\n",
    "    del df_primary_object_baseline\n",
    "else:\n",
    "    print(\n",
    "        \"Could not find objects baselines csv and, thus, cannot append this information to the dataframe\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_not_excluded_with_expert_baseline[\n",
    "    \"correct_center\"\n",
    "] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "    lambda row: row[\"max_query_center_distance\"] > row[\"min_query_center_distance\"],\n",
    "    axis=1,\n",
    ")\n",
    "df_main_not_excluded_with_expert_baseline[\n",
    "    \"correct_std\"\n",
    "] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "    lambda row: row[\"max_query_patch_std\"] < row[\"min_query_patch_std\"], axis=1\n",
    ")\n",
    "df_main_not_excluded_with_expert_baseline[\n",
    "    \"correct_primary\"\n",
    "] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "    lambda row: True if row[\"primary_object_baseline_choice\"] == 1 else False, axis=1\n",
    ")\n",
    "df_main_not_excluded_with_expert_baseline[\n",
    "    \"correct_saliency\"\n",
    "] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "    lambda row: row[\"max_query_patch_saliency\"] < row[\"min_query_patch_saliency\"],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_mode_list = [\n",
    "    \"w_optimized\",\n",
    "    \"w_natural\",\n",
    "    \"w_mixed\",\n",
    "    \"w_blur\",\n",
    "    \"w_none\",\n",
    "    \"b_center\",\n",
    "    #\"b_primary\",\n",
    "    \"b_std\",\n",
    "    \"b_saliency\",\n",
    "]\n",
    "\n",
    "cohens_kappa = utf_helper.get_cohens_kappa_all_conditions_with_each_other(\n",
    "    df_main_not_excluded_with_expert_baseline, extended_mode_list, \"mode_extended\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-carbon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extended_mode_label_dict = {}\n",
    "extended_mode_label_dict[\"w_optimized\"] = \"Synthetic\"\n",
    "extended_mode_label_dict[\"w_natural\"] = \"Natural\"\n",
    "extended_mode_label_dict[\"w_mixed\"] = \"Mixed\"\n",
    "extended_mode_label_dict[\"w_blur\"] = \"Blur\"\n",
    "extended_mode_label_dict[\"w_none\"] = \"None\"\n",
    "extended_mode_label_dict[\"b_center\"] = \"Center\"\n",
    "# extended_mode_label_dict[\"b_primary\"] = \"Object\"\n",
    "extended_mode_label_dict[\"b_std\"] = \"Variance\"\n",
    "extended_mode_label_dict[\"b_saliency\"] = \"Saliency\"\n",
    "\n",
    "utf_mturk.sub_plot_cohens_kappa_by_batch(\n",
    "    cohens_kappa,\n",
    "    extended_mode_list,\n",
    "    extended_mode_label_dict,\n",
    "    figures_folder,\n",
    "    exp_str,\n",
    "    save_fig=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-crime",
   "metadata": {},
   "source": [
    "# Figure 16: Relative Activation Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel_size_i in sorted(df_main_not_excluded[\"kernel_size\"].unique()):\n",
    "    utf_mturk.plot_binned_accuracy_vs_relative_activation_difference(\n",
    "        df_main_not_excluded[df_main_not_excluded[\"kernel_size\"] == kernel_size_i],\n",
    "        figures_folder,\n",
    "        save_fig=True,\n",
    "        fig_name_suffix=f\"_kernel_size{kernel_size_i}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-brooklyn",
   "metadata": {},
   "source": [
    "# Figure 17: Exclusion Criteria (Distribution over results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf_mturk.plot_exclusion_criteria(\n",
    "    df_checks, proportion=False, results_folder=figures_folder, save_fig=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf_mturk.plot_task_postings(\n",
    "    df_checks, proportion=False, results_folder=figures_folder, save_fig=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-fundamentals",
   "metadata": {},
   "source": [
    "# Figure 18: Exclusion Criteria (Distribution over values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-panama",
   "metadata": {},
   "source": [
    "### Included Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf_mturk.plot_instruction_time_details_extracted(\n",
    "    df_checks_not_excluded.copy(), True, figures_folder, save_fig\n",
    ")\n",
    "\n",
    "utf_mturk.plot_total_response_time_details_extracted(\n",
    "    df_checks_not_excluded.copy(), True, figures_folder, save_fig\n",
    ")\n",
    "\n",
    "utf_mturk.plot_catch_trials_details_ratio_exctracted(\n",
    "    df_checks_not_excluded.copy(), True, figures_folder, save_fig\n",
    ")\n",
    "\n",
    "utf_mturk.plot_row_variability_details_upper_extracted(\n",
    "    df_checks_not_excluded.copy(), True, figures_folder, save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-wichita",
   "metadata": {},
   "source": [
    "### Excluded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf_mturk.plot_instruction_time_details_extracted(\n",
    "    df_checks_excluded.copy(), False, figures_folder, save_fig\n",
    ")\n",
    "\n",
    "utf_mturk.plot_total_response_time_details_extracted(\n",
    "    df_checks_excluded.copy(), False, figures_folder, save_fig\n",
    ")\n",
    "\n",
    "utf_mturk.plot_catch_trials_details_ratio_exctracted(\n",
    "    df_checks_excluded.copy(), False, figures_folder, save_fig\n",
    ")\n",
    "\n",
    "utf_mturk.plot_row_variability_details_upper_extracted(\n",
    "    df_checks_excluded.copy(), False, figures_folder, save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf_mturk.plot_practice_trials_attempts(\n",
    "    df_checks[df_checks[\"mode\"] != \"none\"],\n",
    "    proportion=False,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae7caa9d414f390d95db249dd017fe5c7bf8d5b32a211fc0f424d9e916f98819"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
