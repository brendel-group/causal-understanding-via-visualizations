{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates the plots for the main paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import from mturk folder\n",
    "import os, sys, inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "mturkdir = os.path.join(os.path.dirname(os.path.dirname(currentdir)), \"mturk\")\n",
    "sys.path.insert(0, mturkdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mturk import RepeatedTaskResult\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_figures as utf\n",
    "import utils_figures_helper as utf_helper\n",
    "import utils_MTurk_figures as utf_mturk\n",
    "import utils_data as utd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder containing the pkl files generated by the experiment's code\n",
    "raw_results_folder = \"data/counterfactual_experiment\"\n",
    "\n",
    "save_csv = True\n",
    "include_baselines = True\n",
    "\n",
    "ignore_duplicate_participants = False # set to False when using data that has no duplicates.\n",
    "\n",
    "# START for figures\n",
    "save_fig = False\n",
    "# name of the folder in ./figures/ where all resulting figures will be saved\n",
    "exp_str = \"counterfactual_experiment\"\n",
    "instr_type_list = [\"optimized\", \"natural\", \"mixed\", \"blur\", \"none\"]\n",
    "branches_labels_list = [\"3x3\", \"pool\"]\n",
    "kernel_size_list = [\"1\", \"3\"]\n",
    "# END for figures\n",
    "\n",
    "# START for payment\n",
    "mturk_payment_one_HIT = 2.34\n",
    "mturk_payment_one_HIT_none = 0.84\n",
    "repetition_factor_due_to_exclusion = 1.35\n",
    "expected_distinct_workers = 50\n",
    "# END for payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_labels = {\n",
    "    \"optimized\": \"Synthetic\",\n",
    "    \"natural\": \"Natural\",\n",
    "    \"mixed\": \"Mixed\",\n",
    "    \"none\": \"None\",\n",
    "    \"blur\": \"Blur\",\n",
    "}\n",
    "labels = [instruction_labels[it] for it in instr_type_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    os.makedirs(os.path.join(\"figures\", exp_str), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data & preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if the `calculate_relative_activation_difference.py` script was already run for all json configurations\n",
    "If not, do so.\n",
    "to add the query activation information to the structure\n",
    "save the resulting files with the filenames shown below\n",
    "\"\"\"\n",
    "\n",
    "if include_baselines:\n",
    "    structure_json_map = {\n",
    "        \"natural\": \"natural_with_baselines.json\",\n",
    "        \"optimized\": \"optimized_with_baselines.json\",\n",
    "        \"mixed\": \"mixed_with_baselines.json\",\n",
    "        \"blur\": \"natural_blur_with_baselines.json\",\n",
    "        \"none\": \"natural_with_baselines.json\",\n",
    "    }\n",
    "else:\n",
    "    structure_json_map = {\n",
    "        \"natural\": \"natural.json\",\n",
    "        \"optimized\": \"optimized.json\",\n",
    "        \"mixed\": \"mixed.json\",\n",
    "        \"blur\": \"natural_blur.json\",\n",
    "        \"none\": \"natural.json\",\n",
    "    }\n",
    "\n",
    "trial_structures = utd.load_and_parse_trial_structure(\n",
    "    raw_results_folder, [structure_json_map[it] for it in instr_type_list]\n",
    ")\n",
    "trial_structures = {k: v for k, v in zip(instr_type_list, trial_structures)}\n",
    "df, df_checks, df_feedback = utd.load_and_parse_all_results(\n",
    "    raw_results_folder, instr_type_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add a column to the result df indicating whether the row belongs to an excluded or included response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_map_excluded_responses(column_name=\"passed_checks\"):\n",
    "    def map_excluded_responses(row):\n",
    "        rows = df_checks[\n",
    "            (df_checks[\"task_id\"] == row[\"task_id\"])\n",
    "            & (df_checks[\"response_index\"] == row[\"response_index\"])\n",
    "        ]\n",
    "        result = not rows[column_name].item()\n",
    "        return result\n",
    "\n",
    "    return map_excluded_responses\n",
    "\n",
    "\n",
    "df[\"excluded_response\"] = df.apply(get_map_excluded_responses(\"passed_checks\"), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a unique column based on task id and response id (unique within each task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df, df_checks = utd.add_task_response_id(df, df_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_main = (\n",
    "    df[(df[\"catch_trial\"] == False) & (df[\"is_demo\"] == False)]\n",
    "    .reset_index()\n",
    "    .drop(\"index\", axis=1)\n",
    ")\n",
    "df_catch_trials = (\n",
    "    df[(df[\"catch_trial\"] == True) & (df[\"is_demo\"] == False)]\n",
    "    .reset_index()\n",
    "    .drop(\"index\", axis=1)\n",
    ")\n",
    "df_demo_trials = df[df[\"is_demo\"] == True].reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Append structure information such as layer, kernel size, etc. to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_main = utd.append_trial_structure_to_results(df_main, trial_structures)\n",
    "df_catch_trials = utd.append_trial_structure_to_results(\n",
    "    df_catch_trials, trial_structures\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ignore_duplicate_participants:\n",
    "    df_duplicate_tasks = pd.read_csv(\n",
    "        os.path.join(raw_results_folder, \"duplicate_tasks.csv\")\n",
    "    )\n",
    "    df_main[\"excluded_response\"] = df_main.apply(\n",
    "        axis=1,\n",
    "        func=lambda row: True\n",
    "        if row[\"excluded_response\"]\n",
    "        else (\n",
    "            len(\n",
    "                df_duplicate_tasks[\n",
    "                    (df_duplicate_tasks[\"mode\"] == row[\"mode\"])\n",
    "                    & (df_duplicate_tasks[\"task_number\"] == row[\"task_number\"])\n",
    "                ]\n",
    "            )\n",
    "            > 0\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split data up in trials belonging to excluded responses, and those that passed the exclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_excluded = df_main[df_main[\"excluded_response\"]]\n",
    "df_main_not_excluded = df_main[~df_main[\"excluded_response\"]]\n",
    "\n",
    "df_catch_trials_excluded = df_catch_trials[df_catch_trials[\"excluded_response\"]]\n",
    "df_catch_trials_not_excluded = df_catch_trials[~df_catch_trials[\"excluded_response\"]]\n",
    "\n",
    "df_demo_trials_excluded = df_demo_trials[df_demo_trials[\"excluded_response\"]]\n",
    "df_demo_trials_not_excluded = df_demo_trials[~df_demo_trials[\"excluded_response\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how often the demo trials had to be repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checks = utd.checks_add_demo_trial_repetitions(df_demo_trials, df_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df, df_checks = utd.process_checks(df, df_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catch_trials_not_excluded_ignoring_catch_trials = utd.get_catch_trials_as_main_data(\n",
    "    df_catch_trials, df_checks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_checks_not_excluded = df_checks[df_checks[\"passed_checks\"]]\n",
    "df_checks_excluded = df_checks[~df_checks[\"passed_checks\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_csv:\n",
    "    # save dataframes to csv\n",
    "    df_checks.to_csv(os.path.join(raw_results_folder, \"df_exclusion_criteria.csv\"))\n",
    "    df.to_csv(os.path.join(raw_results_folder, \"df_trials.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figures_folder = os.path.join(\n",
    "    \"figures\", os.path.basename(os.path.realpath(raw_results_folder))\n",
    ")\n",
    "if save_fig:\n",
    "    os.makedirs(figures_folder, exist_ok=True)\n",
    "    print(\"Saving results to\", figures_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expert_baseline = pd.read_csv(\"data/baselines/df_main_trials.csv\")\n",
    "df_expert_baseline[\"expert_baseline\"] = True\n",
    "df_main_not_excluded_copy = df_main_not_excluded.copy()\n",
    "df_main_not_excluded_copy[\"expert_baseline\"] = False\n",
    "df_main_not_excluded_with_expert_baseline = pd.concat((df_expert_baseline, df_main_not_excluded_copy)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "utf.make_plot_workers_understood_task(\n",
    "    df_main_not_excluded_with_expert_baseline,\n",
    "    figures_folder,\n",
    "    exp_str,\n",
    "    [\"optimized\", \"natural\", \"none\"],\n",
    "    [\"Synthetic\", \"Natural\", \"None\"],\n",
    "    fig_1=True,\n",
    "    include_experts=False,\n",
    "    save_fig=save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3A: Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utf.make_plot_workers_understood_task(\n",
    "    df_main_not_excluded_with_expert_baseline,\n",
    "    figures_folder,\n",
    "    exp_str,\n",
    "    instr_type_list,\n",
    "    labels,\n",
    "    fig_1=False,\n",
    "    save_fig=save_fig\n",
    ")\n",
    "\n",
    "\n",
    "del df_main_not_excluded_with_expert_baseline, df_main_not_excluded_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3B: Reaction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utf.make_plot_natural_are_better_wrt_reaction_time(\n",
    "    df_main_not_excluded,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=save_fig,\n",
    "    instr_type_list=instr_type_list,\n",
    "    labels=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4A: Baseline Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expert data\n",
    "df_expert_baseline = pd.read_csv(\"data/baselines/df_main_trials.csv\")\n",
    "df_expert_baseline[\"expert_baseline\"] = True\n",
    "df_expert_baseline[\"mode_extended\"] = df_expert_baseline.apply(\n",
    "    lambda row: \"e_\" + row[\"mode\"], axis=1  # e for expert\n",
    ")\n",
    "df_expert_baseline[\"kernel_size\"] = df_expert_baseline.apply(\n",
    "    lambda row: str(row[\"kernel_size\"]), axis=1\n",
    ")\n",
    "df_expert_baseline[\"layer\"] = df_expert_baseline.apply(\n",
    "    lambda row: str(row[\"layer\"]), axis=1\n",
    ")\n",
    "\n",
    "# extend worker df with new columns\n",
    "df_main_not_excluded_copy = df_main_not_excluded.copy()\n",
    "df_main_not_excluded_copy[\"expert_baseline\"] = False\n",
    "df_main_not_excluded_copy[\"mode_extended\"] = df_main_not_excluded_copy.apply(\n",
    "    lambda row: \"w_\" + row[\"mode\"], axis=1  # w for worker\n",
    ")\n",
    "df_main_not_excluded_with_expert_baseline = pd.concat(\n",
    "    (df_expert_baseline, df_main_not_excluded_copy)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# load primary object baseline\n",
    "if os.path.exists(\"data/baselines/df_primary_object_baseline.csv\"):\n",
    "    df_primary_object_baseline = pd.read_csv(\n",
    "        \"data/baselines2/df_primary_object_baseline.csv\"\n",
    "    )\n",
    "\n",
    "    def parse_primary_object_baseline(row):\n",
    "        mask = (\n",
    "            (df_primary_object_baseline[\"batch\"] == row[\"batch\"])\n",
    "            & (df_primary_object_baseline[\"layer\"] == row[\"layer\"])\n",
    "            & (df_primary_object_baseline[\"kernel_size\"] == row[\"kernel_size\"])\n",
    "        )\n",
    "        selected_rows = df_primary_object_baseline[mask]\n",
    "        if not len(selected_rows) == 1:\n",
    "            print(\n",
    "                \"missing information for row:\",\n",
    "                row[[\"batch\", \"trial_index\", \"mode\", \"task_number\"]],\n",
    "            )\n",
    "            print()\n",
    "        return selected_rows.iloc[0][\"primary_object_choice\"]\n",
    "\n",
    "    df_main_not_excluded_with_expert_baseline[\n",
    "        \"primary_object_baseline_choice\"\n",
    "    ] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "        axis=1, func=parse_primary_object_baseline\n",
    "    )\n",
    "\n",
    "    # clean up\n",
    "    del df_primary_object_baseline\n",
    "else:\n",
    "    print(\n",
    "        \"Could not find objects baselines csv and, thus, cannot append this information to the dataframe\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_not_excluded_with_expert_baseline[\n",
    "    \"correct_center\"\n",
    "] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "    lambda row: row[\"max_query_center_distance\"] > row[\"min_query_center_distance\"],\n",
    "    axis=1,\n",
    ")\n",
    "df_main_not_excluded_with_expert_baseline[\n",
    "    \"correct_std\"\n",
    "] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "    lambda row: row[\"max_query_patch_std\"] < row[\"min_query_patch_std\"], axis=1\n",
    ")\n",
    "df_main_not_excluded_with_expert_baseline[\n",
    "    \"correct_primary\"\n",
    "] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "    lambda row: True if row[\"primary_object_baseline_choice\"] == 1 else False, axis=1\n",
    ")\n",
    "df_main_not_excluded_with_expert_baseline[\n",
    "    \"correct_saliency\"\n",
    "] = df_main_not_excluded_with_expert_baseline.apply(\n",
    "    lambda row: row[\"max_query_patch_saliency\"] < row[\"min_query_patch_saliency\"],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracies = {\n",
    "    \"Center\": df_main_not_excluded_with_expert_baseline[\n",
    "        ~df_main_not_excluded_with_expert_baseline[\"expert_baseline\"] # only taking dataframe from workers\n",
    "    ][\"correct_center\"].mean(),\n",
    "    \"Variance\": df_main_not_excluded_with_expert_baseline[\n",
    "        ~df_main_not_excluded_with_expert_baseline[\"expert_baseline\"]\n",
    "    ][\"correct_std\"].mean(),\n",
    "    \"Object\": 0.6344107407407408,\n",
    "    \"Saliency\": df_main_not_excluded_with_expert_baseline[\n",
    "        ~df_main_not_excluded_with_expert_baseline[\"expert_baseline\"]\n",
    "    ][\"correct_saliency\"].mean(),\n",
    "}\n",
    "\n",
    "baseline_sems = {\n",
    "    \"Object\": 0.006435863163504224,\n",
    "}\n",
    "\n",
    "utf.plot_baseline_accuracy(\n",
    "    baseline_accuracies,\n",
    "    sems=baseline_sems,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=save_fig,\n",
    "    label_order=[\"Center\", \"Object\", \"Variance\", \"Saliency\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4 B and C: Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extended_mode_list = [\n",
    "    \"w_optimized\",\n",
    "    \"w_natural\",\n",
    "    \"w_mixed\",\n",
    "    \"w_blur\",\n",
    "    \"w_none\",\n",
    "    \"b_center\",\n",
    "    \"b_std\",\n",
    "    \"b_saliency\",\n",
    "]\n",
    "\n",
    "cohens_kappa = utf_helper.get_cohens_kappa_all_conditions_with_each_other(\n",
    "    df_main_not_excluded_with_expert_baseline, extended_mode_list, \"mode_extended\"\n",
    ")\n",
    "(\n",
    "    cohens_kappa_matrix,\n",
    "    cohens_kappa_std_matrix,\n",
    "    cohens_kappa_sem_matrix,\n",
    ") = utf_helper.get_cohens_kappa_matrix_all_conditions_with_each_other(\n",
    "    extended_mode_list, cohens_kappa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_mode_label_list = [\n",
    "    \"Synthetic\",\n",
    "    \"Natural\",\n",
    "    \"Mixed\",\n",
    "    \"Blur\",\n",
    "    \"None\",\n",
    "    \"Center\\nBaseline\",\n",
    "    \"Variance\\nBaseline\",\n",
    "    \"Saliency\\nBaseline\",\n",
    "]\n",
    "\n",
    "extended_mode_list = [\n",
    "    \"w_optimized\",\n",
    "    \"w_natural\",\n",
    "    \"w_mixed\",\n",
    "    \"w_blur\",\n",
    "    \"w_none\",\n",
    "    \"b_center\",\n",
    "    \"b_std\",\n",
    "    \"b_saliency\",\n",
    "]\n",
    "\n",
    "utf_mturk.plot_worker_baseline_consistency_matrix(\n",
    "    np.round(cohens_kappa_matrix * 100, 2).astype(int),\n",
    "    np.round(2 * cohens_kappa_sem_matrix * 100, 2).astype(int),\n",
    "    extended_mode_list,\n",
    "    extended_mode_label_list,\n",
    "    figures_folder,\n",
    "    save_fig=save_fig,\n",
    "    vmin=cohens_kappa_matrix.min() * 100,\n",
    "    vmax=cohens_kappa_matrix.max() * 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_mode_list = [\"w_optimized\", \"w_natural\", \"b_saliency\"]\n",
    "\n",
    "(\n",
    "    cohens_kappa_submatrix,\n",
    "    cohens_kappa_std_submatrix,\n",
    "    cohens_kappa_sem_submatrix,\n",
    ") = utf_helper.get_cohens_kappa_matrix_all_conditions_with_each_other(\n",
    "    extended_mode_list, cohens_kappa\n",
    ")\n",
    "\n",
    "extended_mode_label_list = [\"Synthetic\", \"Natural\", \"Saliency\\nBaseline\"]\n",
    "\n",
    "utf_mturk.plot_worker_baseline_consistency_matrix(\n",
    "    np.round(cohens_kappa_submatrix * 100, 2).astype(int),\n",
    "    np.round(2 * cohens_kappa_sem_submatrix * 100, 2).astype(int),\n",
    "    extended_mode_list,\n",
    "    extended_mode_label_list,\n",
    "    figures_folder,\n",
    "    save_fig=save_fig,\n",
    "    vmin=cohens_kappa_matrix.min() * 100,\n",
    "    vmax=cohens_kappa_matrix.max() * 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5A, B: Performance by Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel_size_i in df_main_not_excluded[\"kernel_size\"].unique():\n",
    "    print(f\"kernel_size {kernel_size_i}\")\n",
    "    utf_mturk.plot_accuracy_per_layer(\n",
    "        df_main_not_excluded[df_main_not_excluded[\"kernel_size\"] == kernel_size_i],\n",
    "        results_folder=figures_folder,\n",
    "        save_fig=save_fig,\n",
    "        instr_type_list=instr_type_list,\n",
    "        title_prefix=f\"For kernel size {kernel_size_i}: \",\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # include error bars by setting show_sem=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5C\n",
    "### Stop using the catch trials as exclusion criterion and plot the, thus, unbiased performance over these trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utf_mturk.plot_accuracy_per_layer(\n",
    "    df_catch_trials_not_excluded_ignoring_catch_trials,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=save_fig,\n",
    "    instr_type_list=instr_type_list,\n",
    "    legend=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 7: Relative Activation Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel_size_i in sorted(df_main_not_excluded[\"kernel_size\"].unique()):\n",
    "    utf_mturk.plot_accuracy_vs_relative_activation_difference(\n",
    "        df_main_not_excluded[df_main_not_excluded[\"kernel_size\"] == kernel_size_i],\n",
    "        results_folder=figures_folder,\n",
    "        save_fig=save_fig,\n",
    "        fig_name_suffix=f\"_kernel_size{kernel_size_i}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae7caa9d414f390d95db249dd017fe5c7bf8d5b32a211fc0f424d9e916f98819"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
