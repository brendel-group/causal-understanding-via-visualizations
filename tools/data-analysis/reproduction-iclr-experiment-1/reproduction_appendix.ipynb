{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook produces figures related to the intervention-prediction experiment shown in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import from mturk folder\n",
    "import os, sys, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "mturkdir = os.path.join(os.path.dirname(os.path.dirname(currentdir)), \"mturk\")\n",
    "sys.path.insert(0, mturkdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mturk import RepeatedTaskResult\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_ICLR_figures as ut\n",
    "import utils_ICLR_figures_helper as ut_helper\n",
    "import utils_MTurk_figures as ut_mturk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = \"data/intervention_experiment/\"\n",
    "mturk_payment_one_HIT = 1.50\n",
    "repetition_factor_due_to_exclusion = 1.2\n",
    "save_csv = False\n",
    "save_fig = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_results(data_folder):\n",
    "    \"\"\"Load experiment results as pickled RepeatedTaskResult object\"\"\"\n",
    "    result_fns = glob(os.path.join(data_folder, \"result_task_*.pkl\"))\n",
    "\n",
    "    all_results = []\n",
    "    for result_fn in result_fns:\n",
    "        with open(result_fn, \"rb\") as f:\n",
    "            result = pickle.load(f)\n",
    "        if len(result) == 1:\n",
    "            all_results += result\n",
    "        else:\n",
    "            all_results.append(result)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def parse_results(tasks, mode=\"natural\"):\n",
    "    \"\"\"Convert list of RepeatedTaskResult objects to pandas dataframe\"\"\"\n",
    "    dfs = []\n",
    "    for i_task, task_data in enumerate(tasks):\n",
    "        dfs_per_task = []\n",
    "\n",
    "        # take the last response only, as this was the accepted one\n",
    "        response_data = task_data.responses[-1]\n",
    "        response_df = pd.DataFrame(response_data[\"main_data\"]) # if you want look at the demo trials and other raw data, load pd.DataFrame(response_data[\"raw_data\"])\n",
    "        dfs_per_task.append(response_df)\n",
    "\n",
    "        task_df = pd.concat(dfs_per_task, 0)\n",
    "        task_df[\"task_number\"] = i_task\n",
    "        dfs.append(task_df)\n",
    "\n",
    "    df = pd.concat(dfs, 0)\n",
    "\n",
    "    df[\"mode\"] = mode\n",
    "    df = df.reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_check_results(tasks, mode=\"natural\"):\n",
    "    df = []\n",
    "    for task in tasks:\n",
    "        for response_idx, response in enumerate(task.raw_responses):\n",
    "            check_results = response[\"check_results\"]\n",
    "            df.append({\n",
    "                \"task_id\": task.task_id,\n",
    "                \"response_index\": response_idx,\n",
    "                \"passed_checks\": response[\"passed_checks\"],\n",
    "                \"worker_id\": task[1][response_idx][\"worker_id\"],\n",
    "                **{f\"{k}_result\": check_results[k][0] for k in check_results},\n",
    "                **{f\"{k}_details\": check_results[k][1] for k in check_results},\n",
    "            })\n",
    "    df = pd.DataFrame(df)\n",
    "    df[\"mode\"] = mode\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_and_parse_all_results(base_folder):\n",
    "    results_natural = load_results(os.path.join(base_folder, \"natural\"))\n",
    "    results_optimized = load_results(os.path.join(base_folder, \"optimized\"))\n",
    "\n",
    "    df_checks_natural = parse_check_results(results_natural, \"natural\")\n",
    "    df_checks_optimized = parse_check_results(results_optimized, \"optimized\")\n",
    "    df_checks = pd.concat((df_checks_natural, df_checks_optimized)).reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "    df_natural = parse_results(results_natural, \"natural\")\n",
    "    df_optimized= parse_results(results_optimized, \"optimized\")\n",
    "    df = pd.concat((df_natural, df_optimized)).reset_index().drop(\"index\", axis=1)\n",
    "    df[\"corrected_trial_index\"] = df.trial_index - df.trial_index.min()\n",
    "\n",
    "    return df, df_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_parse_trial_structure(folder):\n",
    "    def parse_trials_structure(trials):\n",
    "        results = []\n",
    "        for trial in trials:\n",
    "            query_path = trial[\"queries\"]\n",
    "            parts = query_path.split(\"/\")\n",
    "            batch = parts[-1].split(\"_\")[-1]\n",
    "            channel = parts[-3].split(\"_\")[-1]\n",
    "            kernel_size = parts[-4].split(\"_\")[-1]\n",
    "            layer = parts[-5].split(\"_\")[-1]\n",
    "\n",
    "            results.append(dict(batch=batch, channel=channel, kernel_size=kernel_size, layer=layer))\n",
    "        return results\n",
    "\n",
    "    with open(os.path.join(folder, \"natural.json\"), \"r\") as f:\n",
    "        raw_structure = json.load(f)\n",
    "\n",
    "    structure = {}\n",
    "    for item in raw_structure[\"tasks\"]:\n",
    "        structure[item[\"index\"]] = {k:parse_trials_structure(item[k]) for k in item if k != \"index\"}\n",
    "\n",
    "    return structure\n",
    "\n",
    "def append_trial_structure_to_results(df, structure):\n",
    "    df = df.copy(deep=True)\n",
    "\n",
    "    # merge structure with df\n",
    "    batch_column = []\n",
    "    channel_column = []\n",
    "    kernel_size_column = []\n",
    "    layer_column = []\n",
    "    for i in range(len(df)):\n",
    "        task_number = df.task_number[i] + 1\n",
    "        trial_number = df.corrected_trial_index[i]\n",
    "        info = structure[task_number][\"trials\"][trial_number]\n",
    "        batch_column.append(info[\"batch\"])\n",
    "        channel_column.append(info[\"channel\"])\n",
    "        kernel_size_column.append(info[\"kernel_size\"])\n",
    "        layer_column.append(info[\"layer\"])\n",
    "\n",
    "    df[\"batch\"] = batch_column\n",
    "    df[\"channel\"] = channel_column\n",
    "    df[\"kernel_size\"] = kernel_size_column\n",
    "    df[\"layer\"] = layer_column\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df, df_checks = load_and_parse_all_results(results_folder)\n",
    "trial_structure = load_and_parse_trial_structure(results_folder)\n",
    "df = append_trial_structure_to_results(df, trial_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_main = df[(df[\"catch_trial\"] == False) & (df[\"is_demo\"] == False)]\n",
    "df_catch_trials = df[(df[\"catch_trial\"] == True) & (df[\"is_demo\"] == False)]\n",
    "df_demo_trials = df[df[\"is_demo\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checks[\"instruction_time_details_extracted\"] = df_checks.apply(\n",
    "    lambda row: row[\"instruction_time_details\"][\"total_time\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checks[\"total_response_time_details_extracted\"] = df_checks.apply(\n",
    "    lambda row: row[\"total_response_time_details\"][\"total_time\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checks[\"row_variability_details_details_upper_extracted\"] = df_checks.apply(\n",
    "    lambda row: row[\"row_variability_details\"][\"n_upper_row\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checks[\"row_variability_details_details_lower_extracted\"] = df_checks.apply(\n",
    "    lambda row: row[\"row_variability_details\"][\"n_lower_row\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checks[\"catch_trials_details_ratio_extracted\"] = df_checks.apply(\n",
    "    lambda row: row[\"catch_trials_details\"][\"ratio\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checks[\"catch_trials_details_correctly_answered_extracted\"] = df_checks.apply(\n",
    "    lambda row: row[\"catch_trials_details\"][\"correctly_answered\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_csv:\n",
    "    # save dataframes to csv\n",
    "    df_checks.to_csv(os.path.join(results_folder, \"df_exclusion_criteria.csv\"))\n",
    "    df.to_csv(os.path.join(results_folder, \"df_trials.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Unique Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique_tasks = df_checks.shape[0]\n",
    "n_unique_workers = len(df_checks[\"worker_id\"].unique())\n",
    "print(f\"We analyzed {n_unique_tasks} unique tasks\")\n",
    "print(f\"We had {n_unique_workers} unique workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data compatible with ICLR visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_for_ICLR_analysis = df_main.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename a few columns to make them compatible with the ICLR code\n",
    "df_main_for_ICLR_analysis.rename(columns={\"mode\": \"instr_type\"}, inplace=True)\n",
    "df_main_for_ICLR_analysis.rename(columns={\"task_number\": \"subject_id\"}, inplace=True)\n",
    "df_main_for_ICLR_analysis.rename(columns={\"confidence\": \"abs_conf_rating\"}, inplace=True)\n",
    "df_main_for_ICLR_analysis.rename(columns={\"rt\": \"RT\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figures_folder = os.path.join(\"figures\", os.path.basename(os.path.realpath(results_folder)))\n",
    "os.makedirs(figures_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.make_plot_synthetic_imgs_are_helpful(\n",
    "    df_main_for_ICLR_analysis, \n",
    "    figures_folder,\n",
    "    exp_str=\"full_exp_ks_1\",\n",
    "    save_fig=save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.make_plot_natural_are_better_wrt_reaction_time(\n",
    "    df_main_for_ICLR_analysis, \n",
    "    figures_folder,\n",
    "    exp_str=\"full_exp_ks_1\",\n",
    "    conditioned_on_correctness=True,\n",
    "    save_fig=save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.make_plot_natural_are_better_wrt_reaction_time(\n",
    "    df_main_for_ICLR_analysis, \n",
    "    figures_folder,\n",
    "    exp_str=\"full_exp_ks_1\",\n",
    "    save_fig=save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.make_plot_natural_are_better_wrt_reaction_time(\n",
    "    df_main_for_ICLR_analysis, \n",
    "    figures_folder,\n",
    "    exp_str=\"full_exp_ks_1\",\n",
    "    conditioned_on_falseness=True,\n",
    "    save_fig=save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.make_plot_natural_are_better_wrt_confidence(\n",
    "    df_main_for_ICLR_analysis, \n",
    "    figures_folder,\n",
    "    exp_str=\"full_exp_ks_1\",\n",
    "    conditioned_on_correctness=True,\n",
    "    save_fig=save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.make_plot_natural_are_better_wrt_confidence(\n",
    "    df_main_for_ICLR_analysis, \n",
    "    figures_folder,\n",
    "    exp_str=\"full_exp_ks_1\",\n",
    "    save_fig=save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.make_plot_natural_are_better_wrt_confidence(\n",
    "    df_main_for_ICLR_analysis, \n",
    "    figures_folder,\n",
    "    exp_str=\"full_exp_ks_1\",\n",
    "    conditioned_on_falseness=True,\n",
    "    save_fig=save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 20: Exclusion Criteria (Distribution over results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(ut_mturk)\n",
    "\n",
    "ut_mturk.plot_task_postings(\n",
    "    df_checks,\n",
    "    proportion=False,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Exclusion Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(ut_mturk)\n",
    "importlib.reload(ut_helper)\n",
    "\n",
    "ut_mturk.plot_exclusion_criteria(\n",
    "    df_checks,\n",
    "    proportion=False,\n",
    "    results_folder=figures_folder,\n",
    "    save_fig=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_exclusion_criteria(df_checks):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    rects = plt.bar(np.arange(2), [\n",
    "        (len(df_checks) - df_checks[\"passed_checks\"].sum())/len(df_checks),\n",
    "        df_checks[\"passed_checks\"].sum()/len(df_checks)\n",
    "    ])\n",
    "    ut_helper.autolabel_counts(rects, ax)\n",
    "    plt.xticks(np.arange(2), [\"Failed\", \"Passed\"])\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"All Exclusion Criteria\")\n",
    "    plt.ylabel(\"Proportion of responses\")\n",
    "    plt.show()\n",
    "\n",
    "    for criterion in [c for c in df_checks.columns if c.endswith(\"_result\")]:\n",
    "        fig, ax = plt.subplots(1)\n",
    "        rects = plt.bar(np.arange(2), [\n",
    "            (len(df_checks) - df_checks[criterion].sum())/len(df_checks),\n",
    "            df_checks[criterion].sum()/len(df_checks)\n",
    "        ])\n",
    "        ut_helper.autolabel_counts(rects, ax)\n",
    "        plt.xticks(np.arange(2), [\"Failed\", \"Passed\"])\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"Exclusion Criterion: {criterion.replace('_result','').replace('_', ' ').title()}\")\n",
    "        plt.ylabel(\"Proportion of responses\")\n",
    "        plt.show()\n",
    "\n",
    "plot_exclusion_criteria(df_checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 21: Exclusion Criteria (Distribution over values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Exclusion Criteria for Included Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_passed_checks = df_checks[df_checks[\"passed_checks\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_mturk.plot_instruction_time_details_extracted(\n",
    "    df_passed_checks, \n",
    "    True,\n",
    "    figures_folder, \n",
    "    save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_mturk.plot_total_response_time_details_extracted(\n",
    "    df_passed_checks, \n",
    "    True,\n",
    "    figures_folder, \n",
    "    save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_mturk.plot_catch_trials_details_ratio_extracted(\n",
    "    df_passed_checks, \n",
    "    True,\n",
    "    figures_folder, \n",
    "    save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; importlib.reload(ut_mturk)\n",
    "\n",
    "ut_mturk.plot_row_variability_details_upper_extracted(\n",
    "    df_passed_checks, \n",
    "    True,\n",
    "    figures_folder, \n",
    "    save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Exclusion Criteria for Excluded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failed_checks = df_checks[df_checks[\"passed_checks\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_mturk.plot_instruction_time_details_extracted(\n",
    "    df_failed_checks,\n",
    "    False,\n",
    "    figures_folder, \n",
    "    save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_mturk.plot_total_response_time_details_extracted(\n",
    "    df_failed_checks, \n",
    "    False,\n",
    "    figures_folder, \n",
    "    save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ut_mturk.plot_catch_trials_details_ratio_extracted(\n",
    "    df_failed_checks, \n",
    "    False,\n",
    "    figures_folder, \n",
    "    save_fig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_mturk.plot_row_variability_details_upper_extracted(\n",
    "    df_failed_checks, \n",
    "    False,\n",
    "    figures_folder, \n",
    "    save_fig\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae7caa9d414f390d95db249dd017fe5c7bf8d5b32a211fc0f424d9e916f98819"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
